<!doctype html>
<html class="no-js" lang="en" data-content_root="../">
  <head><meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width,initial-scale=1"/>
    <meta name="color-scheme" content="light dark"><meta name="viewport" content="width=device-width, initial-scale=1" />
<link rel="index" title="Index" href="../genindex.html" /><link rel="search" title="Search" href="../search.html" />

    <!-- Generated with Sphinx 8.1.3 and Furo 2024.08.06 -->
        <title>curobo.rollout.dynamics_model.integration_utils module - cuRobo</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo.css?v=354aac6f" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/furo-extensions.css?v=302659d7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom_furo.css?v=a13d9812" />
    
    


<style>
  body {
    --color-code-background: #f8f8f8;
  --color-code-foreground: black;
  --color-api-pre-name: #76b900;
  --color-api-name: #76b900;
  --color-admonition-title--seealso: #ffffff;
  --color-admonition-title-background--seealso: #448aff;
  --color-admonition-title-background--note: #76b900;
  --color-admonition-title--note: #ffffff;
  --color-brand-primary: #76b900;
  --color-sidebar-background: #f5fff5;
  
  }
  @media not print {
    body[data-theme="dark"] {
      --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  --color-brand-primary: #76b900;
  --color-brand-content: #76b900;
  --color-sidebar-background: #000000;
  
    }
    @media (prefers-color-scheme: dark) {
      body:not([data-theme="light"]) {
        --color-code-background: #202020;
  --color-code-foreground: #d0d0d0;
  --color-admonition-title-background--note: #535353;
  --color-admonition-title--note: #ffffff;
  --color-brand-primary: #76b900;
  --color-brand-content: #76b900;
  --color-sidebar-background: #000000;
  
      }
    }
  }
</style></head>
  <body>
    
    <script>
      document.body.dataset.theme = localStorage.getItem("theme") || "auto";
    </script>
    

<svg xmlns="http://www.w3.org/2000/svg" style="display: none;">
  <symbol id="svg-toc" viewBox="0 0 24 24">
    <title>Contents</title>
    <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 1024 1024">
      <path d="M408 442h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8zm-8 204c0 4.4 3.6 8 8 8h480c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8H408c-4.4 0-8 3.6-8 8v56zm504-486H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zm0 632H120c-4.4 0-8 3.6-8 8v56c0 4.4 3.6 8 8 8h784c4.4 0 8-3.6 8-8v-56c0-4.4-3.6-8-8-8zM115.4 518.9L271.7 642c5.8 4.6 14.4.5 14.4-6.9V388.9c0-7.4-8.5-11.5-14.4-6.9L115.4 505.1a8.74 8.74 0 0 0 0 13.8z"/>
    </svg>
  </symbol>
  <symbol id="svg-menu" viewBox="0 0 24 24">
    <title>Menu</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-menu">
      <line x1="3" y1="12" x2="21" y2="12"></line>
      <line x1="3" y1="6" x2="21" y2="6"></line>
      <line x1="3" y1="18" x2="21" y2="18"></line>
    </svg>
  </symbol>
  <symbol id="svg-arrow-right" viewBox="0 0 24 24">
    <title>Expand</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather-chevron-right">
      <polyline points="9 18 15 12 9 6"></polyline>
    </svg>
  </symbol>
  <symbol id="svg-sun" viewBox="0 0 24 24">
    <title>Light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="feather-sun">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg>
  </symbol>
  <symbol id="svg-moon" viewBox="0 0 24 24">
    <title>Dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-moon">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M12 3c.132 0 .263 0 .393 0a7.5 7.5 0 0 0 7.92 12.446a9 9 0 1 1 -8.313 -12.454z" />
    </svg>
  </symbol>
  <symbol id="svg-sun-with-moon" viewBox="0 0 24 24">
    <title>Auto light/dark, in light mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path style="opacity: 50%" d="M 5.411 14.504 C 5.471 14.504 5.532 14.504 5.591 14.504 C 3.639 16.319 4.383 19.569 6.931 20.352 C 7.693 20.586 8.512 20.551 9.25 20.252 C 8.023 23.207 4.056 23.725 2.11 21.184 C 0.166 18.642 1.702 14.949 4.874 14.536 C 5.051 14.512 5.231 14.5 5.411 14.5 L 5.411 14.504 Z"/>
      <line x1="14.5" y1="3.25" x2="14.5" y2="1.25"/>
      <line x1="14.5" y1="15.85" x2="14.5" y2="17.85"/>
      <line x1="10.044" y1="5.094" x2="8.63" y2="3.68"/>
      <line x1="19" y1="14.05" x2="20.414" y2="15.464"/>
      <line x1="8.2" y1="9.55" x2="6.2" y2="9.55"/>
      <line x1="20.8" y1="9.55" x2="22.8" y2="9.55"/>
      <line x1="10.044" y1="14.006" x2="8.63" y2="15.42"/>
      <line x1="19" y1="5.05" x2="20.414" y2="3.636"/>
      <circle cx="14.5" cy="9.55" r="3.6"/>
    </svg>
  </symbol>
  <symbol id="svg-moon-with-sun" viewBox="0 0 24 24">
    <title>Auto light/dark, in dark mode</title>
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round"
      class="icon-custom-derived-from-feather-sun-and-tabler-moon">
      <path d="M 8.282 7.007 C 8.385 7.007 8.494 7.007 8.595 7.007 C 5.18 10.184 6.481 15.869 10.942 17.24 C 12.275 17.648 13.706 17.589 15 17.066 C 12.851 22.236 5.91 23.143 2.505 18.696 C -0.897 14.249 1.791 7.786 7.342 7.063 C 7.652 7.021 7.965 7 8.282 7 L 8.282 7.007 Z"/>
      <line style="opacity: 50%" x1="18" y1="3.705" x2="18" y2="2.5"/>
      <line style="opacity: 50%" x1="18" y1="11.295" x2="18" y2="12.5"/>
      <line style="opacity: 50%" x1="15.316" y1="4.816" x2="14.464" y2="3.964"/>
      <line style="opacity: 50%" x1="20.711" y1="10.212" x2="21.563" y2="11.063"/>
      <line style="opacity: 50%" x1="14.205" y1="7.5" x2="13.001" y2="7.5"/>
      <line style="opacity: 50%" x1="21.795" y1="7.5" x2="23" y2="7.5"/>
      <line style="opacity: 50%" x1="15.316" y1="10.184" x2="14.464" y2="11.036"/>
      <line style="opacity: 50%" x1="20.711" y1="4.789" x2="21.563" y2="3.937"/>
      <circle style="opacity: 50%" cx="18" cy="7.5" r="2.169"/>
    </svg>
  </symbol>
  <symbol id="svg-pencil" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-pencil-code">
      <path d="M4 20h4l10.5 -10.5a2.828 2.828 0 1 0 -4 -4l-10.5 10.5v4" />
      <path d="M13.5 6.5l4 4" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
  <symbol id="svg-eye" viewBox="0 0 24 24">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="none" stroke="currentColor"
      stroke-width="1" stroke-linecap="round" stroke-linejoin="round" class="icon-tabler-eye-code">
      <path stroke="none" d="M0 0h24v24H0z" fill="none" />
      <path d="M10 12a2 2 0 1 0 4 0a2 2 0 0 0 -4 0" />
      <path
        d="M11.11 17.958c-3.209 -.307 -5.91 -2.293 -8.11 -5.958c2.4 -4 5.4 -6 9 -6c3.6 0 6.6 2 9 6c-.21 .352 -.427 .688 -.647 1.008" />
      <path d="M20 21l2 -2l-2 -2" />
      <path d="M17 17l-2 2l2 2" />
    </svg>
  </symbol>
</svg>

<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation">
<input type="checkbox" class="sidebar-toggle" name="__toc" id="__toc">
<label class="overlay sidebar-overlay" for="__navigation">
  <div class="visually-hidden">Hide navigation sidebar</div>
</label>
<label class="overlay toc-overlay" for="__toc">
  <div class="visually-hidden">Hide table of contents sidebar</div>
</label>

<a class="skip-to-content muted-link" href="#furo-main-content">Skip to content</a>



<div class="page">
  <header class="mobile-header">
    <div class="header-left">
      <label class="nav-overlay-icon" for="__navigation">
        <div class="visually-hidden">Toggle site navigation sidebar</div>
        <i class="icon"><svg><use href="#svg-menu"></use></svg></i>
      </label>
    </div>
    <div class="header-center">
      <a href="../index.html"><div class="brand">cuRobo</div></a>
    </div>
    <div class="header-right">
      <div class="theme-toggle-container theme-toggle-header">
        <button class="theme-toggle">
          <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
          <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
          <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
          <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
          <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
        </button>
      </div>
      <label class="toc-overlay-icon toc-header-icon" for="__toc">
        <div class="visually-hidden">Toggle table of contents sidebar</div>
        <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
      </label>
    </div>
  </header>
  <aside class="sidebar-drawer">
    <div class="sidebar-container">
      
      <div class="sidebar-sticky"><a class="sidebar-brand" href="../index.html">
  
  <div class="sidebar-logo-container">
    <img class="sidebar-logo only-light" src="../_static/logo-light-mode.png" alt="Light Logo"/>
    <img class="sidebar-logo only-dark" src="../_static/logo-dark-mode.png" alt="Dark Logo"/>
  </div>
  
  <span class="sidebar-brand-text">cuRobo</span>
  
</a><form class="sidebar-search-container" method="get" action="../search.html" role="search">
  <input class="sidebar-search" placeholder="Search" name="q" aria-label="Search">
  <input type="hidden" name="check_keywords" value="yes">
  <input type="hidden" name="area" value="default">
</form>
<div id="searchbox"></div><div class="sidebar-scroll"><div class="sidebar-tree">
  <ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../research_index.html">Research</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" role="switch" type="checkbox"/><label for="toctree-checkbox-1"><div class="visually-hidden">Toggle navigation of Research</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../research/research.html">Technical Report</a></li>
<li class="toctree-l2"><a class="reference internal" href="../research/research_using_curobo.html">Research using cuRobo</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../get_started_index.html">Get Started</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" role="switch" type="checkbox"/><label for="toctree-checkbox-2"><div class="visually-hidden">Toggle navigation of Get Started</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../get_started/1_install_instructions.html">Installation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/1a_quick_overview.html">Basics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/2a_python_examples.html">Using in Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/2b_isaacsim_examples.html">Using with Isaac Sim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/2c_world_collision.html">Collision World Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/2d_nvblox_demo.html">Using with Depth Camera</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/2e_torch_layer_example.html">Using in a Neural Network</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/4_benchmarks.html">Benchmarks &amp; Profiling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/5_docker_development.html">Docker Development</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/6_known_issues.html">Known Issues</a></li>
<li class="toctree-l2"><a class="reference internal" href="../get_started/7_api.html">Python API</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../advanced_examples_index.html">Advanced Examples</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" role="switch" type="checkbox"/><label for="toctree-checkbox-3"><div class="visually-hidden">Toggle navigation of Advanced Examples</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../advanced_examples/1_batch_env.html">Batched Environments</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced_examples/2_block_stacking_example.html">Block Stacking</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced_examples/3_constrained_planning.html">Constrained Planning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced_examples/4_robot_segmentation.html">Robot Segmentation</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials_index.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" role="switch" type="checkbox"/><label for="toctree-checkbox-4"><div class="visually-hidden">Toggle navigation of Tutorials</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/1_robot_configuration.html">Configuring a New Robot</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1 has-children"><a class="reference internal" href="../developer_notes_index.html">Developer Notes</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" role="switch" type="checkbox"/><label for="toctree-checkbox-5"><div class="visually-hidden">Toggle navigation of Developer Notes</div><i class="icon"><svg><use href="#svg-arrow-right"></use></svg></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../notes/01_robot_list.html">Supported Robots</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/02_numerical_optimization.html">Numerical Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/03_performance_tuning.html">Performance Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/04_cuda_kernels.html">CUDA Notes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/05_usd_api.html">USD for Robot and World Representation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../notes/07_environment_variables.html">Environment Variables</a></li>
</ul>
</li>
</ul>

</div>
</div>

      </div>
      
    </div>
  </aside>
  <div class="main">
    <div class="content">
      <div class="article-container">
        <a href="#" class="back-to-top muted-link">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
            <path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"></path>
          </svg>
          <span>Back to top</span>
        </a>
        <div class="content-icon-container">
          <div class="theme-toggle-container theme-toggle-content">
            <button class="theme-toggle">
              <div class="visually-hidden">Toggle Light / Dark / Auto color theme</div>
              <svg class="theme-icon-when-auto-light"><use href="#svg-sun-with-moon"></use></svg>
              <svg class="theme-icon-when-auto-dark"><use href="#svg-moon-with-sun"></use></svg>
              <svg class="theme-icon-when-dark"><use href="#svg-moon"></use></svg>
              <svg class="theme-icon-when-light"><use href="#svg-sun"></use></svg>
            </button>
          </div>
          <label class="toc-overlay-icon toc-content-icon" for="__toc">
            <div class="visually-hidden">Toggle table of contents sidebar</div>
            <i class="icon"><svg><use href="#svg-toc"></use></svg></i>
          </label>
        </div>
        <article role="main" id="furo-main-content">
          <section id="module-curobo.rollout.dynamics_model.integration_utils">
<span id="curobo-rollout-dynamics-model-integration-utils-module"></span><h1>curobo.rollout.dynamics_model.integration_utils module<a class="headerlink" href="#module-curobo.rollout.dynamics_model.integration_utils" title="Link to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.build_clique_matrix">
<span class="sig-name descname"><span class="pre">build_clique_matrix</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">horizon</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.float32</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.build_clique_matrix" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.build_fd_matrix">
<span class="sig-name descname"><span class="pre">build_fd_matrix</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">horizon</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.float32</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">PREV_STATE</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">FULL_RANK</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">SHIFT</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.build_fd_matrix" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.build_int_matrix">
<span class="sig-name descname"><span class="pre">build_int_matrix</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">horizon</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">diagonal</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'cpu'</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dtype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">torch.float32</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">order</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.build_int_matrix" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.build_start_state_mask">
<span class="sig-name descname"><span class="pre">build_start_state_mask</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">horizon</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">tensor_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="curobo.types.base.html#curobo.types.base.TensorDeviceType" title="curobo.types.base.TensorDeviceType"><span class="pre">TensorDeviceType</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.build_start_state_mask" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_jerk">
<span class="sig-name descname"><span class="pre">tensor_step_jerk</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dt_h</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_dofs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_jerk" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.euler_integrate">
<span class="sig-name descname"><span class="pre">euler_integrate</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">q_0</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">diag_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.euler_integrate" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_acc">
<span class="sig-name descname"><span class="pre">tensor_step_acc</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dt_h</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_dofs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_acc" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique_contiguous">
<span class="sig-name descname"><span class="pre">jit_tensor_step_pos_clique_contiguous</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">pos_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_2</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_3</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique_contiguous" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique">
<span class="sig-name descname"><span class="pre">jit_tensor_step_pos_clique</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">pos_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_2</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_3</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique">
<span class="sig-name descname"><span class="pre">jit_backward_pos_clique</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_j</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_2</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_3</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique_contiguous">
<span class="sig-name descname"><span class="pre">jit_backward_pos_clique_contiguous</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_j</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_2</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_3</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique_contiguous" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStep</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_mask</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_1</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_2</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_3</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStepKernel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStepIdxKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_idx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStepCentralDifferenceKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_idx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">CliqueTensorStepCoalesceKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AccelerationTensorStepKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">AccelerationTensorStepKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">AccelerationTensorStepIdxKernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel" title="Link to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Function</span></code></a></p>
<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">u_act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">start_idx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_position</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_velocity</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_acceleration</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_jerk</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">traj_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">out_grad_position</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="Link to this definition">¶</a></dt>
<dd><p>Define the forward of the custom autograd Function.</p>
<p>This function is to be overridden by all subclasses.
There are two ways to define forward:</p>
<p>Usage 1 (Combined forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#combining-forward-context" title="(in PyTorch v2.5)"><span>Combined or separate forward() and setup_context()</span></a> for more details</p></li>
</ul>
<p>Usage 2 (Separate forward and ctx):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">forward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">Any</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>

<span class="err">@</span><span class="n">staticmethod</span>
<span class="n">def</span><span class="w"> </span><span class="n">setup_context</span><span class="p">(</span><span class="n">ctx</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">:</span><span class="w"> </span><span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span><span class="w"> </span><span class="p">...],</span><span class="w"> </span><span class="n">output</span><span class="o">:</span><span class="w"> </span><span class="n">Any</span><span class="p">)</span><span class="w"> </span><span class="o">-&gt;</span><span class="w"> </span><span class="n">None</span><span class="o">:</span>
<span class="w">    </span><span class="n">pass</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The forward no longer accepts a ctx argument.</p></li>
<li><p>Instead, you must also override the <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.setup_context</span></code>
staticmethod to handle setting up the <code class="docutils literal notranslate"><span class="pre">ctx</span></code> object.
<code class="docutils literal notranslate"><span class="pre">output</span></code> is the output of the forward, <code class="docutils literal notranslate"><span class="pre">inputs</span></code> are a Tuple of inputs
to the forward.</p></li>
<li><p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details</p></li>
</ul>
<p>The context can be used to store arbitrary data that can be then
retrieved during the backward pass. Tensors should not be stored
directly on <cite>ctx</cite> (though this is not currently enforced for
backward compatibility). Instead, tensors should be saved either with
<code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_backward</span></code> if they are intended to be used in
<code class="docutils literal notranslate"><span class="pre">backward</span></code> (equivalently, <code class="docutils literal notranslate"><span class="pre">vjp</span></code>) or <code class="xref py py-func docutils literal notranslate"><span class="pre">ctx.save_for_forward</span></code>
if they are intended to be used for in <code class="docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_p</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_v</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_a</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">grad_out_j</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._backward_cls">
<span class="sig-name descname"><span class="pre">_backward_cls</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._backward_cls" title="Link to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernelBackward</span></code></p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_backward_state">
<span class="sig-name descname"><span class="pre">_compiled_autograd_backward_state</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_backward_state" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_key">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_compiled_autograd_key</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_key" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._get_compiled_autograd_symints">
<span class="sig-name descname"><span class="pre">_get_compiled_autograd_symints</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._get_compiled_autograd_symints" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._input_metadata">
<span class="sig-name descname"><span class="pre">_input_metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._input_metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._is_compiled_autograd_tracing">
<span class="sig-name descname"><span class="pre">_is_compiled_autograd_tracing</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._is_compiled_autograd_tracing" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._materialize_non_diff_grads">
<span class="sig-name descname"><span class="pre">_materialize_non_diff_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._materialize_non_diff_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._raw_saved_tensors">
<span class="sig-name descname"><span class="pre">_raw_saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._raw_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">_register_hook</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">backward_hooks</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook_dict">
<span class="sig-name descname"><span class="pre">_register_hook_dict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook_dict" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._sequence_nr">
<span class="sig-name descname"><span class="pre">_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._set_sequence_nr">
<span class="sig-name descname"><span class="pre">_set_sequence_nr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._set_sequence_nr" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.apply">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.apply" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.dirty_tensors">
<span class="sig-name descname"><span class="pre">dirty_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.dirty_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.generate_vmap_rule">
<span class="sig-name descname"><span class="pre">generate_vmap_rule</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">False</span></em><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.generate_vmap_rule" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">jvp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with forward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many inputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> got (None will be passed in
for non tensor inputs of the forward function),
and it should return as many tensors as there were outputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given input,
and each returned value should be the gradient w.r.t. the
corresponding output. If an output is not a Tensor or the function is not
differentiable with respect to that output, you can just pass None as a
gradient for that input.</p>
<p>You can use the <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> object to pass any value from the forward to this
functions.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_dirty">
<span class="sig-name descname"><span class="pre">mark_dirty</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_dirty" title="Link to this definition">¶</a></dt>
<dd><p>Mark given tensors as modified in an in-place operation.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be inputs.</p>
<p>Every tensor that’s been modified in-place in a call to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>
should be given to this function, to ensure correctness of our checks.
It doesn’t matter whether the function is called before or after
modification.</p>
<dl>
<dt>Examples::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Inplace</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># x_npy shares storage with x</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x_npy</span> <span class="o">+=</span> <span class="mi">1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_dirty</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_output</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">*</span> <span class="n">a</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Inplace</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># This would lead to wrong gradients!</span>
<span class="gp">&gt;&gt;&gt; </span>                  <span class="c1"># but the engine would not know unless we mark_dirty</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># RuntimeError: one of the variables needed for gradient</span>
<span class="gp">&gt;&gt;&gt; </span>             <span class="c1"># computation has been modified by an inplace operation</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_non_differentiable">
<span class="sig-name descname"><span class="pre">mark_non_differentiable</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_non_differentiable" title="Link to this definition">¶</a></dt>
<dd><p>Mark outputs as non-differentiable.</p>
<p>This should be called at most once, in either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a>
or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments should be tensor outputs.</p>
<p>This will mark outputs as not requiring gradients, increasing the
efficiency of backward computation. You still need to accept a gradient
for each output in <code class="xref py py-meth docutils literal notranslate"><span class="pre">backward</span></code>, but it’s always going to
be a zero tensor with the same shape as the shape of a corresponding
output.</p>
<dl>
<dt>This is used e.g. for indices returned from a sort. See example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">mark_non_differentiable</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="nb">sorted</span><span class="p">,</span> <span class="n">idx</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>  <span class="c1"># still need to accept g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span><span class="o">.</span><span class="n">index_add_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">g1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_shared_storage">
<span class="sig-name descname"><span class="pre">mark_shared_storage</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">pairs</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_shared_storage" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.materialize_grads">
<span class="sig-name descname"><span class="pre">materialize_grads</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.materialize_grads" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.maybe_clear_saved_tensors">
<span class="sig-name descname"><span class="pre">maybe_clear_saved_tensors</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.maybe_clear_saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.metadata">
<span class="sig-name descname"><span class="pre">metadata</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.metadata" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.name">
<span class="sig-name descname"><span class="pre">name</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.name" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.needs_input_grad">
<span class="sig-name descname"><span class="pre">needs_input_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.needs_input_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.next_functions">
<span class="sig-name descname"><span class="pre">next_functions</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.next_functions" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.non_differentiable">
<span class="sig-name descname"><span class="pre">non_differentiable</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.non_differentiable" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_hook">
<span class="sig-name descname"><span class="pre">register_hook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_hook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_prehook">
<span class="sig-name descname"><span class="pre">register_prehook</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_prehook" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.requires_grad">
<span class="sig-name descname"><span class="pre">requires_grad</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.requires_grad" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_backward">
<span class="sig-name descname"><span class="pre">save_for_backward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_backward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and only with tensors.</p>
<p>All tensors intended to be used in the backward pass should be saved
with <code class="docutils literal notranslate"><span class="pre">save_for_backward</span></code> (as opposed to directly on <code class="docutils literal notranslate"><span class="pre">ctx</span></code>) to prevent
incorrect gradients and memory leaks, and enable the application of saved
tensor hooks. See <a class="reference external" href="https://pytorch.org/docs/stable/autograd.html#torch.autograd.graph.saved_tensors_hooks" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.autograd.graph.saved_tensors_hooks</span></code></a>.</p>
<p>Note that if intermediary tensors, tensors that are neither inputs
nor outputs of <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>, are saved for backward, your custom Function
may not support double backward.
Custom Functions that do not support double backward should decorate their
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> method with <code class="docutils literal notranslate"><span class="pre">&#64;once_differentiable</span></code> so that performing
double backward raises an error. If you’d like to support double backward,
you can either recompute intermediaries based on the inputs during backward
or return the intermediaries as the outputs of the custom Function. See the
<a class="reference external" href="https://pytorch.org/tutorials/intermediate/custom_function_double_backward_tutorial.html">double backward tutorial</a>
for more details.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a>, saved tensors can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute. Before returning them to the user, a check is made to ensure
they weren’t used in any in-place operation that modified their content.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">w</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span> <span class="o">*</span> <span class="n">y</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>  <span class="c1"># z is not a tensor</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">out</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">out</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gx</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gy</span> <span class="o">=</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="n">z</span> <span class="o">+</span> <span class="n">w</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">gz</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">gx</span><span class="p">,</span> <span class="n">gy</span><span class="p">,</span> <span class="n">gz</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_forward">
<span class="sig-name descname"><span class="pre">save_for_forward</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">tensors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_forward" title="Link to this definition">¶</a></dt>
<dd><p>Save given tensors for a future call to <code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">save_for_forward</span></code> should be called at most once, in either the
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods, and all arguments
should be tensors.</p>
<p>In <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a>, saved objects can be accessed through the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors"><code class="xref py py-attr docutils literal notranslate"><span class="pre">saved_tensors</span></code></a>
attribute.</p>
<p>Arguments can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>. This is a no-op.</p>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details on how to use this method.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span> <span class="o">*</span> <span class="n">z</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">jvp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x_t</span><span class="p">,</span> <span class="n">y_t</span><span class="p">,</span> <span class="n">_</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">x_t</span> <span class="o">+</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y_t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">vjp</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_out</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">z</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">z</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">z</span> <span class="o">*</span> <span class="n">grad_out</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="kc">None</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">double</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">c</span> <span class="o">=</span> <span class="mi">4</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">with</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">dual_level</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">a_dual</span> <span class="o">=</span> <span class="n">fwAD</span><span class="o">.</span><span class="n">make_dual</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">d</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a_dual</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_for_forward">
<span class="sig-name descname"><span class="pre">saved_for_forward</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_for_forward" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors">
<span class="sig-name descname"><span class="pre">saved_tensors</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_variables">
<span class="sig-name descname"><span class="pre">saved_variables</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_variables" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.set_materialize_grads">
<span class="sig-name descname"><span class="pre">set_materialize_grads</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><span class="pre">bool</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.set_materialize_grads" title="Link to this definition">¶</a></dt>
<dd><p>Set whether to materialize grad tensors. Default is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
<p>This should be called only from either the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="xref py py-func docutils literal notranslate"><span class="pre">setup_context</span></code></a> or
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> methods.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, undefined grad tensors will be expanded to tensors full of zeros
prior to calling the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> and <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp"><code class="xref py py-func docutils literal notranslate"><span class="pre">jvp</span></code></a> methods.</p>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">SimpleFunc</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">g1</span> <span class="o">+</span> <span class="n">g2</span>  <span class="c1"># No check for None necessary</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># We modify SimpleFunc to handle non-materialized grad outputs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Func</span><span class="p">(</span><span class="n">Function</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">set_materialize_grads</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@staticmethod</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nd">@once_differentiable</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">g1</span><span class="p">,</span> <span class="n">g2</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">grad_input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g1</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># We must check for None now</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g1</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">if</span> <span class="n">g2</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>            <span class="n">grad_input</span> <span class="o">+=</span> <span class="n">g2</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">grad_input</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">b</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">Func</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>  <span class="c1"># induces g2 to be undefined</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">setup_context</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.13)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">output</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context" title="Link to this definition">¶</a></dt>
<dd><p>There are two ways to define the forward pass of an autograd.Function.</p>
<p>Either:</p>
<ol class="arabic simple">
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(ctx,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code>.
<code class="docutils literal notranslate"><span class="pre">setup_context</span></code> is not overridden. Setting up the ctx for backward
happens inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
<li><p>Override forward with the signature <code class="docutils literal notranslate"><span class="pre">forward(*args,</span> <span class="pre">**kwargs)</span></code> and
override <code class="docutils literal notranslate"><span class="pre">setup_context</span></code>. Setting up the ctx for backward happens
inside <code class="docutils literal notranslate"><span class="pre">setup_context</span></code> (as opposed to inside the <code class="docutils literal notranslate"><span class="pre">forward</span></code>)</p></li>
</ol>
<p>See <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.Function.forward.html#torch.autograd.Function.forward" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.autograd.Function.forward</span></code></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.html#extending-autograd" title="(in PyTorch v2.5)"><span>Extending torch.autograd</span></a> for more details.</p>
</dd></dl>

<dl class="py attribute">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.to_save">
<span class="sig-name descname"><span class="pre">to_save</span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.to_save" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vjp">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vjp</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">ctx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">grad_outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.13)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vjp" title="Link to this definition">¶</a></dt>
<dd><p>Define a formula for differentiating the operation with backward mode automatic differentiation.</p>
<p>This function is to be overridden by all subclasses.
(Defining this function is equivalent to defining the <code class="docutils literal notranslate"><span class="pre">vjp</span></code> function.)</p>
<p>It must accept a context <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx</span></code> as the first argument, followed by
as many outputs as the <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> returned (None will be passed in
for non tensor outputs of the forward function),
and it should return as many tensors, as there were inputs to
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a>. Each argument is the gradient w.r.t the given output,
and each returned value should be the gradient w.r.t. the
corresponding input. If an input is not a Tensor or is a Tensor not
requiring grads, you can just pass None as a gradient for that input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute <code class="xref py py-attr docutils literal notranslate"><span class="pre">ctx.needs_input_grad</span></code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
<a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="xref py py-func docutils literal notranslate"><span class="pre">backward</span></code></a> will have <code class="docutils literal notranslate"><span class="pre">ctx.needs_input_grad[0]</span> <span class="pre">=</span> <span class="pre">True</span></code> if the
first input to <a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward" title="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward</span></code></a> needs gradient computed w.r.t. the
output.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vmap">
<em class="property"><span class="pre">static</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">vmap</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">info</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">in_dims</span></span></em>,</dd>
<dd><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vmap" title="Link to this definition">¶</a></dt>
<dd><p>Define the behavior for this autograd.Function underneath <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p>
<p>For a <code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.Function</span></code> to support
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>, you must either override this static method, or set
<code class="docutils literal notranslate"><span class="pre">generate_vmap_rule</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> (you may not do both).</p>
<p>If you choose to override this staticmethod: it must accept</p>
<ul class="simple">
<li><p>an <code class="docutils literal notranslate"><span class="pre">info</span></code> object as the first argument. <code class="docutils literal notranslate"><span class="pre">info.batch_size</span></code>
specifies the size of the dimension being vmapped over,
while <code class="docutils literal notranslate"><span class="pre">info.randomness</span></code> is the randomness option passed to
<a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.vmap.html#torch.vmap" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.vmap</span></code></a>.</p></li>
<li><p>an <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> tuple as the second argument.
For each arg in <code class="docutils literal notranslate"><span class="pre">args</span></code>, <code class="docutils literal notranslate"><span class="pre">in_dims</span></code> has a corresponding
<code class="docutils literal notranslate"><span class="pre">Optional[int]</span></code>. It is <code class="docutils literal notranslate"><span class="pre">None</span></code> if the arg is not a Tensor or if
the arg is not being vmapped over, otherwise, it is an integer
specifying what dimension of the Tensor is being vmapped over.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*args</span></code>, which is the same as the args to <code class="xref py py-meth docutils literal notranslate"><span class="pre">forward</span></code>.</p></li>
</ul>
<p>The return of the vmap staticmethod is a tuple of <code class="docutils literal notranslate"><span class="pre">(output,</span> <span class="pre">out_dims)</span></code>.
Similar to <code class="docutils literal notranslate"><span class="pre">in_dims</span></code>, <code class="docutils literal notranslate"><span class="pre">out_dims</span></code> should be of the same structure as
<code class="docutils literal notranslate"><span class="pre">output</span></code> and contain one <code class="docutils literal notranslate"><span class="pre">out_dim</span></code> per output that specifies if the
output has the vmapped dimension and what index it is in.</p>
<p>Please see <a class="reference external" href="https://pytorch.org/docs/stable/notes/extending.func.html#func-autograd-function" title="(in PyTorch v2.5)"><span>Extending torch.func with autograd.Function</span></a> for more details.</p>
</dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_clique">
<span class="sig-name descname"><span class="pre">tensor_step_pos_clique</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="curobo.types.state.html#curobo.types.state.JointState" title="curobo.types.state.JointState"><span class="pre">JointState</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="curobo.types.state.html#curobo.types.state.JointState" title="curobo.types.state.JointState"><span class="pre">JointState</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">mask_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.13)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.5)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_clique" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.step_acc_semi_euler">
<span class="sig-name descname"><span class="pre">step_acc_semi_euler</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">diag_dt</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_dofs</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.step_acc_semi_euler" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_acc_semi_euler">
<span class="sig-name descname"><span class="pre">tensor_step_acc_semi_euler</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">diag_dt</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix_pos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.13)"><span class="pre">None</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_acc_semi_euler" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_vel">
<span class="sig-name descname"><span class="pre">tensor_step_vel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">dt_h</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">n_dofs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">integrate_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_matrix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Tensor</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">Tensor</span></span></span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_vel" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_pos">
<span class="sig-name descname"><span class="pre">tensor_step_pos</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">state</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">act</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">state_seq</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">fd_matrix</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_ik">
<span class="sig-name descname"><span class="pre">tensor_step_pos_ik</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">act</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">state_seq</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_ik" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.tensor_linspace">
<span class="sig-name descname"><span class="pre">tensor_linspace</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">start_tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">end_tensor</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">steps</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">10</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.tensor_linspace" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.sum_matrix">
<span class="sig-name descname"><span class="pre">sum_matrix</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">h</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">int_steps</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor_args</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.sum_matrix" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.interpolate_kernel">
<span class="sig-name descname"><span class="pre">interpolate_kernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">h</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">int_steps</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">tensor_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="curobo.types.base.html#curobo.types.base.TensorDeviceType" title="curobo.types.base.TensorDeviceType"><span class="pre">TensorDeviceType</span></a></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.interpolate_kernel" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="curobo.rollout.dynamics_model.integration_utils.action_interpolate_kernel">
<span class="sig-name descname"><span class="pre">action_interpolate_kernel</span></span><span class="sig-paren">(</span>

<dl>
<dd><em class="sig-param"><span class="n"><span class="pre">h</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">int_steps</span></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">tensor_args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference internal" href="curobo.types.base.html#curobo.types.base.TensorDeviceType" title="curobo.types.base.TensorDeviceType"><span class="pre">TensorDeviceType</span></a></span></em>,</dd>
<dd><em class="sig-param"><span class="n"><span class="pre">offset</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">4</span></span></em>,</dd>
</dl>

<span class="sig-paren">)</span><a class="headerlink" href="#curobo.rollout.dynamics_model.integration_utils.action_interpolate_kernel" title="Link to this definition">¶</a></dt>
<dd></dd></dl>

</section>

        </article>
      </div>
      <footer>
        
        <div class="related-pages">
          
          
        </div>
        <div class="bottom-of-page">
          <div class="left-details">
            <div class="copyright">
                Copyright &#169; 2023-2024, NVIDIA
            </div>
            Made with 
            <a href="https://github.com/pradyunsg/furo">Furo</a>
            
          </div>
          <div class="right-details">
            <div class="icons">
              <a class="muted-link " href="https://github.com/NVLabs/curobo" aria-label="GitHub">
                <svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 16 16">
                    <path fill-rule="evenodd" d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z"></path>
                </svg>
            </a>
              
            </div>
          </div>
        </div>
        
      </footer>
    </div>
    <aside class="toc-drawer">
      
      
      <div class="toc-sticky toc-scroll">
        <div class="toc-title-container">
          <span class="toc-title">
            On this page
          </span>
        </div>
        <div class="toc-tree-container">
          <div class="toc-tree">
            <ul>
<li><a class="reference internal" href="#">curobo.rollout.dynamics_model.integration_utils module</a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.build_clique_matrix"><code class="docutils literal notranslate"><span class="pre">build_clique_matrix</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.build_fd_matrix"><code class="docutils literal notranslate"><span class="pre">build_fd_matrix</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.build_int_matrix"><code class="docutils literal notranslate"><span class="pre">build_int_matrix</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.build_start_state_mask"><code class="docutils literal notranslate"><span class="pre">build_start_state_mask</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_jerk"><code class="docutils literal notranslate"><span class="pre">tensor_step_jerk</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.euler_integrate"><code class="docutils literal notranslate"><span class="pre">euler_integrate</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_acc"><code class="docutils literal notranslate"><span class="pre">tensor_step_acc</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique_contiguous"><code class="docutils literal notranslate"><span class="pre">jit_tensor_step_pos_clique_contiguous</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.jit_tensor_step_pos_clique"><code class="docutils literal notranslate"><span class="pre">jit_tensor_step_pos_clique</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique"><code class="docutils literal notranslate"><span class="pre">jit_backward_pos_clique</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.jit_backward_pos_clique_contiguous"><code class="docutils literal notranslate"><span class="pre">jit_backward_pos_clique_contiguous</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStep.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStep.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepKernel.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxKernel.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCentralDifferenceKernel.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCentralDifferenceKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepIdxCentralDifferenceKernel.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepIdxCentralDifferenceKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.apply"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.jvp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.metadata"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.name"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.to_save"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vjp"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.CliqueTensorStepCoalesceKernel.vmap"><code class="docutils literal notranslate"><span class="pre">CliqueTensorStepCoalesceKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.backward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.apply"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.jvp"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.metadata"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.name"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.to_save"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vjp"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepKernel.vmap"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel</span></code></a><ul>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.backward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._backward_cls"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._backward_cls</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_backward_state"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._compiled_autograd_backward_state</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._compiled_autograd_key"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._compiled_autograd_key</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._get_compiled_autograd_symints"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._get_compiled_autograd_symints</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._input_metadata"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._input_metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._is_compiled_autograd_tracing"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._is_compiled_autograd_tracing</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._materialize_non_diff_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._materialize_non_diff_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._raw_saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._raw_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._register_hook_dict"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._register_hook_dict</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._sequence_nr"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel._set_sequence_nr"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel._set_sequence_nr</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.apply"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.apply</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.dirty_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.dirty_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.generate_vmap_rule"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.generate_vmap_rule</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.jvp"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.jvp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_dirty"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.mark_dirty</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_non_differentiable"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.mark_non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.mark_shared_storage"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.mark_shared_storage</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.materialize_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.maybe_clear_saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.maybe_clear_saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.metadata"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.metadata</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.name"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.name</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.needs_input_grad"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.needs_input_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.next_functions"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.next_functions</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.non_differentiable"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.non_differentiable</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_hook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.register_hook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.register_prehook"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.register_prehook</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.requires_grad"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.requires_grad</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_backward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.save_for_backward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.save_for_forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.save_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_for_forward"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.saved_for_forward</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_tensors"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.saved_tensors</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.saved_variables"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.saved_variables</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.set_materialize_grads"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.set_materialize_grads</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.setup_context"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.setup_context</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.to_save"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.to_save</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vjp"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.vjp</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.AccelerationTensorStepIdxKernel.vmap"><code class="docutils literal notranslate"><span class="pre">AccelerationTensorStepIdxKernel.vmap</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_clique"><code class="docutils literal notranslate"><span class="pre">tensor_step_pos_clique</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.step_acc_semi_euler"><code class="docutils literal notranslate"><span class="pre">step_acc_semi_euler</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_acc_semi_euler"><code class="docutils literal notranslate"><span class="pre">tensor_step_acc_semi_euler</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_vel"><code class="docutils literal notranslate"><span class="pre">tensor_step_vel</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos"><code class="docutils literal notranslate"><span class="pre">tensor_step_pos</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_step_pos_ik"><code class="docutils literal notranslate"><span class="pre">tensor_step_pos_ik</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.tensor_linspace"><code class="docutils literal notranslate"><span class="pre">tensor_linspace</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.sum_matrix"><code class="docutils literal notranslate"><span class="pre">sum_matrix</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.interpolate_kernel"><code class="docutils literal notranslate"><span class="pre">interpolate_kernel</span></code></a></li>
<li><a class="reference internal" href="#curobo.rollout.dynamics_model.integration_utils.action_interpolate_kernel"><code class="docutils literal notranslate"><span class="pre">action_interpolate_kernel</span></code></a></li>
</ul>
</li>
</ul>

          </div>
        </div>
      </div>
      
      
    </aside>
  </div>
</div><script src="../_static/documentation_options.js?v=674563c9"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/furo.js?v=5fa4622c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    </body>
</html>